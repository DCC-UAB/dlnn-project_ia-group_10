{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LSTMModel , LSTMModel_seeOnce\n",
    "from dataset import ImageCaptionDataset\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Júlia Garcia Torné/Desktop/Artificial inteligence/Segon curs/Part 2/Neural Networks and Deep Learning/Project/dlnn-project_ia-group_10/dataset/train_dataset.pkl', 'rb') as inp:\n",
    "    train_dataset = pickle.load(inp)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "#load word2vec pretrained embedding layer\n",
    "word2vec_emb = api.load('word2vec-google-news-300')\n",
    "word2vec_emb = torch.FloatTensor(word2vec_emb.vectors)\n",
    "\n",
    "#import vocabulary to word2vec indexes that we know\n",
    "with open('vocabidx2word2vecidx.pkl', 'rb') as inp:\n",
    "    vocabidx2word2vecidx = pickle.load(inp)\n",
    "\n",
    "with open('vocabulary.pkl', 'rb') as inp:\n",
    "    vocabulary = pickle.load(inp)\n",
    "\n",
    "word2vec_emb = word2vec_emb[vocabidx2word2vecidx]\n",
    "\n",
    "word2vec_emb = nn.Embedding.from_pretrained(word2vec_emb)\n",
    "word2vec_emb.requires_grad_ = False #freeze word2vec embeddding layer\n",
    "\n",
    "#load universal sentence encoder to define our own loss function\n",
    "sntc_enc = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#define cross entropy loss function\n",
    "cross_entrop = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = nn.Embedding(num_embeddings=word2vec_emb.weight.shape[0], embedding_dim=100)\n",
    "\n",
    "model = LSTMModel(input_dim=512,embedding_layer=emb_layer,hidden_dim=100,n_layers=1)\n",
    "X1,X2,caption = next(iter(train_dataloader))\n",
    "out,_,_ = model(X1,X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "dot = make_dot(out)\n",
    "dot.render('computation_graph', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ['Image features', \"Captions\"]\n",
    "output_names = ['Caption',\"Hidden states\",\"Cell states\"]\n",
    "torch.onnx.export(model, (X1,X2), 'lstm.onnx', input_names=input_names, output_names=output_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
