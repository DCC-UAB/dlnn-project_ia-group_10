{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18 finetune to create embeddings similar to GPT2 embedded captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this notebook is to explore and attempt to use contrastive learning to embedd the extracted features of ResNet18 and finetune it for our task.\n",
    "\n",
    "We might want to do two different approaches:\n",
    "- Embedd/process captions and perform k-means to make sure they make sense, then use the distance of two captions of two images as the distance that two images should have(contrastive learning)\n",
    "\n",
    "- Also, once we've proven we can embedd the captions in a way that are distinct by their semantic meaning try to match the image embedding to the caption embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load gpt2 and start playing. Also load the dataset captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "captions = pd.read_csv(\"dataset/captions.txt\")\n",
    "captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return hidden states\n",
    "gpt2.config.output_hidden_states=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First question, how are we going to embedd the captions?\n",
    "\n",
    "We might try to just pass them through gpt2 and the representation before passing it to the LM head (FFC layer) will be our representation of the caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, the task we want to gpt to learn (or the scheme) will be: [image representation as last token (or many tokens)] \"< s  > or some token to start and then the generated caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try processing a caption with GPT2 to get the last hidden representation before passing it to the FFC for LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A child in a pink dress is climbing up a set of stairs in an entry way .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_caption = captions[\"caption\"][0]\n",
    "sample_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| inputs[\"input_ids\"].shape: torch.Size([1, 18])\n",
      "ic| outputs[\"logits\"].shape: torch.Size([1, 18, 50257])\n",
      "ic| len(outputs[\"hidden_states\"]): 13\n",
      "ic| outputs[\"hidden_states\"][-1].shape: torch.Size([1, 18, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_caption, return_tensors=\"pt\")\n",
    "    ic(inputs[\"input_ids\"].shape)\n",
    "    outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "    ic(outputs[\"logits\"].shape) #not last hidden state!\n",
    "    ic(len(outputs[\"hidden_states\"])) #13 layers 12+embedding\n",
    "    ic(outputs[\"hidden_states\"][-1].shape) #last layer\n",
    "    last_hidden = outputs[\"hidden_states\"][-1]\n",
    "    last_tok_hidden = last_hidden[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode all the captions and then perform k-means to see if we can differentiate one between the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = gpt2.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2232/40455 [01:29<25:31, 24.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(END_TOKEN \u001b[38;5;241m+\u001b[39m cpt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m gpt2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m vectors\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    vectors = []\n",
    "    for i,row in tqdm(captions.iterrows(),total=len(captions)):\n",
    "        cpt = row[\"caption\"]\n",
    "        #not sure if everytime we pass something it doesn't do self attention on previously \n",
    "        #passed tokens(in other instances) to make sure it doesn't pass \"<|endoftext|>\"\n",
    "        inputs = tokenizer(END_TOKEN + cpt, return_tensors=\"pt\").to(\"mps\")\n",
    "        outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "        vectors.append(outputs[\"hidden_states\"][-1][:,-1,:].to(\"cpu\").squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pkl file the \"embedded captions\"\n",
    "with open('captions_gpt2_embedded.pkl', 'wb') as file:\n",
    "    pkl.dump(vectors, file, protocol=pkl.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions_gpt2_embedded.pkl', 'rb') as handle:\n",
    "    vectors = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-means to see how much they represent this difference in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/64lnvrmj50q5dv_5dj_ztbz40000gn/T/ipykernel_9983/4079812674.py:2: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(vectors)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tags\n",
       "0       40455\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pd.DataFrame(columns=[\"tags\"])\n",
    "tags[\"tags\"] = kmeans.labels_\n",
    "tags.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 478/40455 [00:54<1:16:22,  8.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v1 \u001b[38;5;129;01min\u001b[39;00m tqdm(vectors):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v2 \u001b[38;5;129;01min\u001b[39;00m vectors:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(vectors[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound two distinc vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2188\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2184\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2185\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n\u001b[0;32m-> 2188\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2190\u001b[0m         initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[39m    Sum of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2193\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[39m    15\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2311\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, _gentype):\n\u001b[1;32m   2312\u001b[0m         \u001b[39m# 2018-02-25, 1.15.0\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for v1 in tqdm(vectors):\n",
    "    for v2 in vectors:\n",
    "        if(np.sum(v1 == v2) < len(vectors[0])):\n",
    "            print(\"found two distinc vectors\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the captions in some clusters same some words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for common words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we'll look into getting some representation a class of the image in a much more hard coded way. We'll lemmatize and apply POS tagging to the captions so we can identifiy the main elements of this very simple captions: Noun - Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A child in a pink dress is climbing up a set of stairs in an entry way.\n",
      "climbing\n",
      "child\n",
      "set\n"
     ]
    }
   ],
   "source": [
    "#example of what we'll do.\n",
    "caption = \"A child in a pink dress is climbing up a set of stairs in an entry way.\"\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "sntc = list(doc.sents)\n",
    "root_token = sntc[0].root\n",
    "for child in root_token.children:\n",
    "    if child.dep_ == 'nsubj':\n",
    "        subj = child\n",
    "    if child.dep_ == 'dobj':\n",
    "        obj = child\n",
    "\n",
    "print(caption)\n",
    "print(root_token)\n",
    "print(subj)\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two men are ice fishing .\n",
      "are\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][99]\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "sntc = list(doc.sents)\n",
    "root_token = sntc[0].root\n",
    "for child in root_token.children:\n",
    "    #in case the root is a verb then get the subject, in case the root is a noun get a verb\n",
    "    if child.pos_  == \"VERB\" or child.dep_ == \"nsubj\" : \n",
    "        baby = child\n",
    "\n",
    "print(caption)\n",
    "print(root_token)\n",
    "print(baby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two different breeds of brown and white dogs play on the beach .\n",
      "play\n",
      "breeds\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][100]\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "sntc = list(doc.sents)\n",
    "root_token = sntc[0].root\n",
    "for child in root_token.children:\n",
    "    #in case the root is a verb then get the subject, in case the root is a noun get a verb\n",
    "    if child.pos_  == \"VERB\" or child.dep_ == \"nsubj\" : \n",
    "        baby = child\n",
    "\n",
    "print(caption)\n",
    "print(root_token)\n",
    "print(baby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler approach, get all verbs and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two different breeds of brown and white dogs play on the beach .\n",
      "['breed', 'dog', 'beach']\n",
      "['play']\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][100]\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "nouns = []\n",
    "verbs = []\n",
    "root_token = sntc[0].root\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token.lemma_)\n",
    "    if token.pos_ == \"VERB\":\n",
    "        verbs.append(token.lemma_)\n",
    "\n",
    "\n",
    "print(caption)\n",
    "print(nouns)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n",
      "['girl', 'paint', 'front', 'rainbow', 'hand', 'bowl']\n",
      "['cover', 'sit', 'paint']\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][10]\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "nouns = []\n",
    "verbs = []\n",
    "root_token = sntc[0].root\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token.lemma_)\n",
    "    if token.pos_ == \"VERB\":\n",
    "        verbs.append(token.lemma_)\n",
    "\n",
    "\n",
    "print(caption)\n",
    "print(nouns)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a brown dog jumping into a pool after a bloe ball .\n",
      "['dog', 'pool', 'bloe', 'ball']\n",
      "['jump']\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][800]\n",
    "doc = nlp(caption) #process sequence\n",
    "#for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "#we might also look for a noun in the object.\n",
    "\n",
    "nouns = []\n",
    "verbs = []\n",
    "root_token = sntc[0].root\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token.lemma_)\n",
    "    if token.pos_ == \"VERB\":\n",
    "        verbs.append(token.lemma_)\n",
    "\n",
    "\n",
    "print(caption)\n",
    "print(nouns)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a class we can embedd the different words using glove and then summ them up to get a representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77069  ,  0.12827  ,  0.33137  ,  0.0050893, -0.47605  ,\n",
       "       -0.50116  ,  1.858    ,  1.0624   , -0.56511  ,  0.13328  ,\n",
       "       -0.41918  , -0.14195  , -2.8555   , -0.57131  , -0.13418  ,\n",
       "       -0.44922  ,  0.48591  , -0.6479   , -0.84238  ,  0.61669  ,\n",
       "       -0.19824  , -0.57967  , -0.65885  ,  0.43928  , -0.50473  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the word \"hello\"\n",
    "word = 'hello'\n",
    "glove_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " '.',\n",
       " ':',\n",
       " 'rt',\n",
       " ',',\n",
       " '<repeat>',\n",
       " '<hashtag>',\n",
       " '<number>',\n",
       " '<url>',\n",
       " '!',\n",
       " 'i',\n",
       " 'a',\n",
       " '\"',\n",
       " 'the',\n",
       " '?',\n",
       " 'you',\n",
       " 'to',\n",
       " '(',\n",
       " '<allcaps>',\n",
       " '<elong>',\n",
       " ')',\n",
       " 'me',\n",
       " 'de',\n",
       " '<smile>',\n",
       " '！',\n",
       " 'que',\n",
       " 'and',\n",
       " '。',\n",
       " '-',\n",
       " 'my',\n",
       " 'no',\n",
       " '、',\n",
       " 'is',\n",
       " 'it',\n",
       " '…',\n",
       " 'in',\n",
       " 'n',\n",
       " 'for',\n",
       " '/',\n",
       " 'of',\n",
       " 'la',\n",
       " \"'s\",\n",
       " '*',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'on',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'e',\n",
       " 'o',\n",
       " 'u',\n",
       " 'en',\n",
       " 'this',\n",
       " 'el',\n",
       " 'so',\n",
       " 'be',\n",
       " \"'m\",\n",
       " 'with',\n",
       " 'just',\n",
       " '>',\n",
       " 'your',\n",
       " '^',\n",
       " 'like',\n",
       " 'have',\n",
       " 'te',\n",
       " 'at',\n",
       " '？',\n",
       " 'love',\n",
       " 'se',\n",
       " 'are',\n",
       " '<',\n",
       " 'm',\n",
       " 'r',\n",
       " 'if',\n",
       " 'all',\n",
       " 'b',\n",
       " '・',\n",
       " 'not',\n",
       " 'but',\n",
       " 'we',\n",
       " 'es',\n",
       " 'ya',\n",
       " '&',\n",
       " 'follow',\n",
       " 'up',\n",
       " 'what',\n",
       " 'get',\n",
       " 'lol',\n",
       " 'un',\n",
       " '♥',\n",
       " 'lo',\n",
       " 'when',\n",
       " 'was',\n",
       " '“',\n",
       " '”',\n",
       " 'one',\n",
       " 'por',\n",
       " 'si',\n",
       " 'out',\n",
       " '_',\n",
       " 'mi',\n",
       " 'can',\n",
       " '<sadface>',\n",
       " 'من',\n",
       " '♡',\n",
       " '´',\n",
       " 'he',\n",
       " 'con',\n",
       " 'they',\n",
       " 'now',\n",
       " 'go',\n",
       " '،',\n",
       " 'para',\n",
       " 'los',\n",
       " 'know',\n",
       " 'haha',\n",
       " 'good',\n",
       " 'tu',\n",
       " 'back',\n",
       " '~',\n",
       " 'about',\n",
       " 'new',\n",
       " ';',\n",
       " 'as',\n",
       " 'day',\n",
       " 'how',\n",
       " 'who',\n",
       " 'will',\n",
       " 'want',\n",
       " 'people',\n",
       " 'yo',\n",
       " 'eu',\n",
       " 'from',\n",
       " 'di',\n",
       " 'time',\n",
       " '<heart>',\n",
       " 's',\n",
       " 'aku',\n",
       " 'da',\n",
       " \"'re\",\n",
       " '<lolface>',\n",
       " 'una',\n",
       " 'got',\n",
       " 'las',\n",
       " 'more',\n",
       " 'x',\n",
       " 'she',\n",
       " 'today',\n",
       " '（',\n",
       " '>>',\n",
       " 'k',\n",
       " 'by',\n",
       " 'or',\n",
       " 'في',\n",
       " '･',\n",
       " 'too',\n",
       " 'le',\n",
       " 'é',\n",
       " '|',\n",
       " '[',\n",
       " '）',\n",
       " ']',\n",
       " 'see',\n",
       " 'why',\n",
       " 'yg',\n",
       " 'ca',\n",
       " 'como',\n",
       " 'her',\n",
       " '—',\n",
       " 'q',\n",
       " 'need',\n",
       " 'an',\n",
       " 'na',\n",
       " '笑',\n",
       " 'there',\n",
       " 'ω',\n",
       " 'happy',\n",
       " 'im',\n",
       " 'mas',\n",
       " 'je',\n",
       " 'life',\n",
       " 'really',\n",
       " 'make',\n",
       " 'yang',\n",
       " 'shit',\n",
       " 'think',\n",
       " 't',\n",
       " '❤',\n",
       " 'não',\n",
       " 'never',\n",
       " 'some',\n",
       " '～',\n",
       " 'oh',\n",
       " '★',\n",
       " 'did',\n",
       " 'would',\n",
       " 'del',\n",
       " '`',\n",
       " 'd',\n",
       " 'please',\n",
       " 'via',\n",
       " 'much',\n",
       " 'fuck',\n",
       " 'al',\n",
       " 'dia',\n",
       " '$',\n",
       " 'و',\n",
       " 'right',\n",
       " 'best',\n",
       " 'c',\n",
       " 'going',\n",
       " 'الله',\n",
       " 'pero',\n",
       " 'only',\n",
       " 'has',\n",
       " '♪',\n",
       " \"'ll\",\n",
       " 'twitter',\n",
       " '=',\n",
       " 'hahaha',\n",
       " 'its',\n",
       " 'nn',\n",
       " '｀',\n",
       " '¿',\n",
       " 'am',\n",
       " 'say',\n",
       " '<neutralface>',\n",
       " 'them',\n",
       " 'here',\n",
       " 'لا',\n",
       " 'off',\n",
       " 'still',\n",
       " 'dan',\n",
       " '+',\n",
       " 'night',\n",
       " 'w',\n",
       " 'ada',\n",
       " 'someone',\n",
       " 'even',\n",
       " 'then',\n",
       " '☆',\n",
       " 'ni',\n",
       " 'come',\n",
       " 'com',\n",
       " 'always',\n",
       " 'man',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'his',\n",
       " 'itu',\n",
       " 'على',\n",
       " '-_-',\n",
       " '☺',\n",
       " 'over',\n",
       " 'um',\n",
       " 'ما',\n",
       " 'hate',\n",
       " 'girl',\n",
       " 'ai',\n",
       " 'had',\n",
       " 'pra',\n",
       " 'todo',\n",
       " 'mais',\n",
       " 'feel',\n",
       " 'let',\n",
       " 'ini',\n",
       " 'because',\n",
       " 'ﾟ',\n",
       " 'thanks',\n",
       " 'ah',\n",
       " 'way',\n",
       " 'ever',\n",
       " 'look',\n",
       " 'tweet',\n",
       " 'followers',\n",
       " 'should',\n",
       " 'our',\n",
       " 'xd',\n",
       " 'aja',\n",
       " 'esta',\n",
       " 'school',\n",
       " 'him',\n",
       " 'ser',\n",
       " 'take',\n",
       " 'than',\n",
       " 'video',\n",
       " 'em',\n",
       " 'last',\n",
       " 'wanna',\n",
       " 'does',\n",
       " 'us',\n",
       " 'miss',\n",
       " 'l',\n",
       " 'ga',\n",
       " 'better',\n",
       " 'well',\n",
       " 'could',\n",
       " '▽',\n",
       " '%',\n",
       " 'apa',\n",
       " 'cuando',\n",
       " 'team',\n",
       " '✔',\n",
       " '@',\n",
       " 'ok',\n",
       " '؟',\n",
       " '•',\n",
       " 'vida',\n",
       " 'quiero',\n",
       " 'les',\n",
       " 'being',\n",
       " 'real',\n",
       " 'down',\n",
       " 'kamu',\n",
       " 'everyone',\n",
       " 'gonna',\n",
       " 'live',\n",
       " 'tonight',\n",
       " 'yes',\n",
       " 'work',\n",
       " 'ass',\n",
       " 'retweet',\n",
       " 'nada',\n",
       " 'sama',\n",
       " 'first',\n",
       " '<<',\n",
       " 'photo',\n",
       " 'tomorrow',\n",
       " 'where',\n",
       " 'god',\n",
       " 'son',\n",
       " 'ke',\n",
       " 'ta',\n",
       " 'f',\n",
       " 'home',\n",
       " 'lagi',\n",
       " 'thank',\n",
       " 'birthday',\n",
       " '█',\n",
       " 'ha',\n",
       " 'great',\n",
       " 'lmao',\n",
       " 'omg',\n",
       " 'morning',\n",
       " 'más',\n",
       " 'mau',\n",
       " 'baby',\n",
       " 'dont',\n",
       " '｡',\n",
       " 'their',\n",
       " 'p',\n",
       " 'things',\n",
       " 'game',\n",
       " 'pas',\n",
       " 'bad',\n",
       " 'year',\n",
       " 'yeah',\n",
       " 'su',\n",
       " 'bitch',\n",
       " 'в',\n",
       " 'stop',\n",
       " 'hoy',\n",
       " 'something',\n",
       " 'meu',\n",
       " 'tak',\n",
       " 'gak',\n",
       " 'world',\n",
       " 'amor',\n",
       " 'h',\n",
       " '\\\\',\n",
       " 'ver',\n",
       " '；',\n",
       " 'porque',\n",
       " 'give',\n",
       " 'these',\n",
       " 'اللهم',\n",
       " 'were',\n",
       " 'hay',\n",
       " 'sleep',\n",
       " 'gue',\n",
       " 'every',\n",
       " 'friends',\n",
       " 'uma',\n",
       " 'tell',\n",
       " 'amo',\n",
       " 'vou',\n",
       " 'bien',\n",
       " '¡',\n",
       " 'again',\n",
       " '＾',\n",
       " '／',\n",
       " 'done',\n",
       " 'after',\n",
       " 'todos',\n",
       " 'girls',\n",
       " 'guys',\n",
       " 'getting',\n",
       " 'big',\n",
       " 'wait',\n",
       " 'justin',\n",
       " 'eh',\n",
       " '→',\n",
       " 'kan',\n",
       " 'kita',\n",
       " 'jajaja',\n",
       " 'wish',\n",
       " 'said',\n",
       " 'fucking',\n",
       " 'show',\n",
       " 'thing',\n",
       " 'next',\n",
       " 'você',\n",
       " 'nos',\n",
       " 'little',\n",
       " 'tengo',\n",
       " 'keep',\n",
       " 'person',\n",
       " \"''\",\n",
       " '∀',\n",
       " 'hope',\n",
       " 'كل',\n",
       " 'hey',\n",
       " 'bisa',\n",
       " 'free',\n",
       " 'made',\n",
       " 'foto',\n",
       " 'va',\n",
       " 'everything',\n",
       " 'iya',\n",
       " 'nigga',\n",
       " 'eso',\n",
       " 'et',\n",
       " 'watch',\n",
       " 'music',\n",
       " 'week',\n",
       " 'talk',\n",
       " 'ne',\n",
       " 'solo',\n",
       " 'gente',\n",
       " 'udah',\n",
       " '：',\n",
       " '--',\n",
       " '＼',\n",
       " 'mejor',\n",
       " 'facebook',\n",
       " 'ma',\n",
       " 'v',\n",
       " 'phone',\n",
       " 'most',\n",
       " 'same',\n",
       " 'okay',\n",
       " 'ik',\n",
       " 'before',\n",
       " 'minha',\n",
       " 'days',\n",
       " 'g',\n",
       " 'ti',\n",
       " 'damn',\n",
       " 'nice',\n",
       " 'voy',\n",
       " 'vai',\n",
       " 'call',\n",
       " 'long',\n",
       " 'tapi',\n",
       " 'http',\n",
       " 'sin',\n",
       " 'nunca',\n",
       " 'doing',\n",
       " 'other',\n",
       " 'find',\n",
       " 'il',\n",
       " 'sa',\n",
       " 'sorry',\n",
       " 'nya',\n",
       " 'orang',\n",
       " '°',\n",
       " 'hard',\n",
       " 'mean',\n",
       " 'die',\n",
       " 'اللي',\n",
       " 'tem',\n",
       " 'soy',\n",
       " 'este',\n",
       " 'kalo',\n",
       " 'só',\n",
       " 'th',\n",
       " 'win',\n",
       " 'nothing',\n",
       " 'into',\n",
       " 'face',\n",
       " 'cute',\n",
       " \"'d\",\n",
       " 'gracias',\n",
       " 'lah',\n",
       " 'и',\n",
       " 'any',\n",
       " 'play',\n",
       " '←',\n",
       " 'ko',\n",
       " 'text',\n",
       " '⌣',\n",
       " 'estoy',\n",
       " 'tau',\n",
       " 'ur',\n",
       " 'buat',\n",
       " '#',\n",
       " 'cause',\n",
       " 'я',\n",
       " 'put',\n",
       " 'kau',\n",
       " 'siempre',\n",
       " 'juga',\n",
       " 'casa',\n",
       " 'أن',\n",
       " 'help',\n",
       " 'start',\n",
       " 'feliz',\n",
       " 'old',\n",
       " 'ir',\n",
       " 'very',\n",
       " 'care',\n",
       " 'bir',\n",
       " 'makes',\n",
       " 'song',\n",
       " 'check',\n",
       " 'watching',\n",
       " 'ahora',\n",
       " 'jadi',\n",
       " 'os',\n",
       " 'may',\n",
       " 'friend',\n",
       " 'beautiful',\n",
       " 'heart',\n",
       " 'ka',\n",
       " 'vc',\n",
       " 'mundo',\n",
       " 'на',\n",
       " 'sure',\n",
       " 'tan',\n",
       " 'pretty',\n",
       " 'aqui',\n",
       " 'не',\n",
       " 'house',\n",
       " 'رتويت',\n",
       " 'يا',\n",
       " 'ja',\n",
       " 'true',\n",
       " 'muy',\n",
       " 'away',\n",
       " 'already',\n",
       " 'actually',\n",
       " 'believe',\n",
       " 'try',\n",
       " 'many',\n",
       " 'mañana',\n",
       " 'mis',\n",
       " 'lu',\n",
       " 'those',\n",
       " 'hot',\n",
       " 'qué',\n",
       " 'mal',\n",
       " 'عن',\n",
       " 'though',\n",
       " 'ask',\n",
       " 'amazing',\n",
       " 'bed',\n",
       " '}',\n",
       " 'two',\n",
       " 'mom',\n",
       " 'día',\n",
       " 've',\n",
       " 'dari',\n",
       " 'gameinsight',\n",
       " 'stay',\n",
       " 'fun',\n",
       " 'around',\n",
       " 'van',\n",
       " 'cont',\n",
       " 'ready',\n",
       " 'money',\n",
       " 'bu',\n",
       " 'funny',\n",
       " 'cool',\n",
       " 'hair',\n",
       " 'à',\n",
       " 'tho',\n",
       " '{',\n",
       " 'wo',\n",
       " 'hi',\n",
       " 'name',\n",
       " 'tiene',\n",
       " 'hahahaha',\n",
       " 'pa',\n",
       " 'algo',\n",
       " 'gotta',\n",
       " 'ولا',\n",
       " 'boy',\n",
       " 'another',\n",
       " \"c'est\",\n",
       " 'hari',\n",
       " 'jajajaja',\n",
       " 'having',\n",
       " 'cara',\n",
       " 'jaja',\n",
       " 'dm',\n",
       " 'looking',\n",
       " 'top',\n",
       " 'android',\n",
       " 'dah',\n",
       " 'wow',\n",
       " '░',\n",
       " 'eres',\n",
       " 'ben',\n",
       " 'must',\n",
       " 'news',\n",
       " 'met',\n",
       " 'está',\n",
       " 'nih',\n",
       " 'family',\n",
       " 'black',\n",
       " 'thought',\n",
       " 'nak',\n",
       " 'super',\n",
       " 'end',\n",
       " 'hace',\n",
       " 'remember',\n",
       " 'ama',\n",
       " 'party',\n",
       " 'cant',\n",
       " 'vamos',\n",
       " 'anything',\n",
       " 'anyone',\n",
       " 'فولو',\n",
       " 'perfect',\n",
       " 'guy',\n",
       " 'vez',\n",
       " 'christmas',\n",
       " 'dos',\n",
       " 'bueno',\n",
       " 'nao',\n",
       " 'years',\n",
       " 'vote',\n",
       " 'dormir',\n",
       " 'bro',\n",
       " 'else',\n",
       " 'quien',\n",
       " 'untuk',\n",
       " 'jangan',\n",
       " 'myself',\n",
       " 'head',\n",
       " 'mind',\n",
       " 'gua',\n",
       " 'talking',\n",
       " 'while',\n",
       " 'dat',\n",
       " 'food',\n",
       " 'д',\n",
       " 'coming',\n",
       " 'wkwk',\n",
       " 'trying',\n",
       " 'saya',\n",
       " 'mucho',\n",
       " 'without',\n",
       " 'wrong',\n",
       " '’s',\n",
       " 'baru',\n",
       " '__',\n",
       " 'hehe',\n",
       " 'hacer',\n",
       " 'lot',\n",
       " 'followed',\n",
       " 'crazy',\n",
       " 'hell',\n",
       " 'feeling',\n",
       " 'des',\n",
       " 'kok',\n",
       " 'j',\n",
       " 'stats',\n",
       " \"j'\",\n",
       " 'ان',\n",
       " 'tweets',\n",
       " 'non',\n",
       " 'cosas',\n",
       " 'era',\n",
       " 'high',\n",
       " 'niggas',\n",
       " 'change',\n",
       " 'movie',\n",
       " 'xx',\n",
       " 'mad',\n",
       " 'sih',\n",
       " 'sometimes',\n",
       " 'deh',\n",
       " 'allah',\n",
       " 'through',\n",
       " 'pour',\n",
       " 'ela',\n",
       " 'soon',\n",
       " 'gone',\n",
       " 'playing',\n",
       " 'smile',\n",
       " 'bukan',\n",
       " 'tv',\n",
       " 'fans',\n",
       " 'hasta',\n",
       " 'akan',\n",
       " \"y'\",\n",
       " 'looks',\n",
       " 'isso',\n",
       " '✌',\n",
       " 'tired',\n",
       " 'boys',\n",
       " 'might',\n",
       " 'dong',\n",
       " 'lg',\n",
       " 'use',\n",
       " 'maybe',\n",
       " 'until',\n",
       " 'menos',\n",
       " 'own',\n",
       " 'dengan',\n",
       " 'eat',\n",
       " 'ou',\n",
       " 'weekend',\n",
       " '˘',\n",
       " 'class',\n",
       " 'ele',\n",
       " 'harry',\n",
       " 'iphone',\n",
       " 'friday',\n",
       " 'single',\n",
       " 'ff',\n",
       " 'awesome',\n",
       " 'bout',\n",
       " 'muito',\n",
       " 'hoje',\n",
       " '¬',\n",
       " 'dios',\n",
       " 'such',\n",
       " 'estar',\n",
       " 'já',\n",
       " 'quando',\n",
       " 'esa',\n",
       " 'making',\n",
       " '━',\n",
       " 'times',\n",
       " 'lmfao',\n",
       " 'gw',\n",
       " 'moment',\n",
       " 'yet',\n",
       " 'aw',\n",
       " 'smh',\n",
       " 'banget',\n",
       " 'masih',\n",
       " 'qui',\n",
       " 'quem',\n",
       " '–',\n",
       " 'leave',\n",
       " 'du',\n",
       " 'une',\n",
       " 'guess',\n",
       " 'hit',\n",
       " 'с',\n",
       " 'pm',\n",
       " 'since',\n",
       " 'pues',\n",
       " 'est',\n",
       " 'job',\n",
       " 'ﾉ',\n",
       " 'mana',\n",
       " 'bom',\n",
       " 'siapa',\n",
       " 'suka',\n",
       " 'bieber',\n",
       " 'mention',\n",
       " 'lebih',\n",
       " 'favorite',\n",
       " 'bitches',\n",
       " 'forever',\n",
       " 'لي',\n",
       " 'final',\n",
       " 'read',\n",
       " 'alguien',\n",
       " 'open',\n",
       " 'yourself',\n",
       " 'ese',\n",
       " 'che',\n",
       " 'sex',\n",
       " 'yaa',\n",
       " 'car',\n",
       " 'direction',\n",
       " 'tidak',\n",
       " 'seu',\n",
       " 'gets',\n",
       " 'left',\n",
       " 're',\n",
       " 'jam',\n",
       " 'enough',\n",
       " 'إلا',\n",
       " 'once',\n",
       " '’',\n",
       " 'part',\n",
       " 'cada',\n",
       " '定期',\n",
       " 'لك',\n",
       " 'een',\n",
       " 'seen',\n",
       " 'kak',\n",
       " 'así',\n",
       " 'nem',\n",
       " 'عمل',\n",
       " 'white',\n",
       " 'told',\n",
       " 'says',\n",
       " 'esto',\n",
       " 'sad',\n",
       " 'mo',\n",
       " 'fue',\n",
       " 'yah',\n",
       " 'summer',\n",
       " 'ه',\n",
       " '⭕',\n",
       " '»',\n",
       " 'thats',\n",
       " 'مع',\n",
       " 'posted',\n",
       " 'wants',\n",
       " 'agora',\n",
       " 'together',\n",
       " 'fan',\n",
       " 'men',\n",
       " 'hear',\n",
       " 'full',\n",
       " '☀',\n",
       " 'sigo',\n",
       " 'pq',\n",
       " 'dulu',\n",
       " 'plus',\n",
       " 'foi',\n",
       " 'tudo',\n",
       " 'هو',\n",
       " 'ill',\n",
       " 'あ',\n",
       " 'thinking',\n",
       " 'wtf',\n",
       " 'pagi',\n",
       " 'mama',\n",
       " 'kalau',\n",
       " 'hati',\n",
       " 'sexy',\n",
       " 'sayang',\n",
       " 'baik',\n",
       " 'semua',\n",
       " 'hola',\n",
       " 'went',\n",
       " 'vos',\n",
       " 'tanto',\n",
       " 'finally',\n",
       " 'fb',\n",
       " 'sea',\n",
       " 'stupid',\n",
       " 'tus',\n",
       " 'seriously',\n",
       " 'hora',\n",
       " 'min',\n",
       " 'pic',\n",
       " 'estas',\n",
       " 'turn',\n",
       " 'hours',\n",
       " 'excited',\n",
       " 'nah',\n",
       " 'buy',\n",
       " 'saying',\n",
       " 'mah',\n",
       " 'break',\n",
       " 'needs',\n",
       " 'ce',\n",
       " 'room',\n",
       " 'choice',\n",
       " 'far',\n",
       " 'dead',\n",
       " 'quero',\n",
       " 'saw',\n",
       " 'kids',\n",
       " 'lil',\n",
       " 'whole',\n",
       " 'puede',\n",
       " 'fall',\n",
       " 'sus',\n",
       " 'lost',\n",
       " 'asi',\n",
       " 'word',\n",
       " '☹',\n",
       " 'also',\n",
       " 'ريتويت',\n",
       " 'probably',\n",
       " 'everybody',\n",
       " 'tarde',\n",
       " 'run',\n",
       " 'sei',\n",
       " 'follback',\n",
       " 'forget',\n",
       " 'sweet',\n",
       " 'welcome',\n",
       " 'selamat',\n",
       " '＿',\n",
       " 'sur',\n",
       " 'place',\n",
       " 'gusta',\n",
       " 'sabe',\n",
       " 'androidgames',\n",
       " 'tp',\n",
       " 'tiempo',\n",
       " 'بس',\n",
       " 'sou',\n",
       " 'tuh',\n",
       " 'vs',\n",
       " 'eyes',\n",
       " 'انا',\n",
       " 'picture',\n",
       " 'das',\n",
       " 'meet',\n",
       " 'anak',\n",
       " 'persona',\n",
       " 'essa',\n",
       " 'bored',\n",
       " 'following',\n",
       " 'nadie',\n",
       " 'nobody',\n",
       " 'dice',\n",
       " 'alone',\n",
       " 'sick',\n",
       " 'red',\n",
       " 'city',\n",
       " 'cinta',\n",
       " '月',\n",
       " 'linda',\n",
       " 'dream',\n",
       " 'story',\n",
       " 'km',\n",
       " 'het',\n",
       " 'waiting',\n",
       " '^_^',\n",
       " 'mine',\n",
       " 'что',\n",
       " 'reason',\n",
       " 'kk',\n",
       " 'لو',\n",
       " 'online',\n",
       " 'fast',\n",
       " 'udh',\n",
       " 'wanted',\n",
       " 'op',\n",
       " 'others',\n",
       " 'gay',\n",
       " 'n’t',\n",
       " 'used',\n",
       " 'sem',\n",
       " 'understand',\n",
       " 'moi',\n",
       " 'sm',\n",
       " 'aint',\n",
       " 'donde',\n",
       " 'bem',\n",
       " 'which',\n",
       " 'ng',\n",
       " 'followback',\n",
       " 'punya',\n",
       " 'late',\n",
       " 'anda',\n",
       " 'tidur',\n",
       " 'puedo',\n",
       " 'early',\n",
       " 'nd',\n",
       " 'personas',\n",
       " 'banyak',\n",
       " '✅',\n",
       " '➊',\n",
       " 'trust',\n",
       " 'noche',\n",
       " 'tl',\n",
       " '＞',\n",
       " '«',\n",
       " 'af',\n",
       " 'move',\n",
       " 'pro',\n",
       " 'bring',\n",
       " 'ku',\n",
       " 'called',\n",
       " 'relationship',\n",
       " 'idk',\n",
       " 'hurt',\n",
       " 'st',\n",
       " 'pernah',\n",
       " 'pessoas',\n",
       " 'hello',\n",
       " 'uno',\n",
       " 'unfollowers',\n",
       " 'cry',\n",
       " ...]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embedd_caption(caption):\n",
    "    doc = nlp(caption) #process sequence\n",
    "    #for each sequence look for the one that is subject and noun and one that is object verb.\n",
    "    #we might also look for a noun in the object.\n",
    "\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    root_token = sntc[0].root\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.append(token.lemma_)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.lemma_)\n",
    "    \n",
    "    nouns_emb = np.array([glove_vectors[word] for word in nouns if word in glove_vectors.index_to_key]) #check if in vocab\n",
    "    verbs_emb = np.array([glove_vectors[word] for word in verbs if word in glove_vectors.index_to_key])\n",
    "\n",
    "\n",
    "    #sum the vectors up but normalising the number of nouns and verbs\n",
    "    nouns_emb /= len(nouns_emb)\n",
    "    verbs_emb /= len(verbs_emb)\n",
    "    \n",
    "    try:\n",
    "        emb  =  np.append(nouns_emb,verbs_emb,axis=0)\n",
    "    except:\n",
    "        if nouns_emb.shape[0] > 0:\n",
    "            emb = nouns_emb\n",
    "        else:\n",
    "            emb = verbs_emb\n",
    "\n",
    "    emb = np.sum(emb,axis=0)\n",
    "\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black dog leaps over a log .\n",
      "[-0.29985    -0.08996     0.1367      0.18855667 -0.04983    -0.30964068\n",
      "  0.55050665 -0.67535007  0.67251664  0.17497666 -0.01235534  0.28218\n",
      " -2.8983665   0.06203665 -0.46037334  0.06152333  0.66373664 -0.08760332\n",
      "  0.08296648 -0.29561666 -0.3776      0.39835998  0.78195333 -0.08756666\n",
      " -0.30947334]\n"
     ]
    }
   ],
   "source": [
    "caption = captions[\"caption\"][45]\n",
    "print(caption)\n",
    "print(glove_embedd_caption(caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40455/40455 [06:42<00:00, 100.63it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i,row in tqdm(captions.iterrows(),total=len(captions)):\n",
    "    caption = row[\"caption\"]\n",
    "    try:\n",
    "        emb = glove_embedd_caption(caption)\n",
    "    except:\n",
    "        print(i)\n",
    "    vectors.append(glove_embedd_caption(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40455,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40455,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pkl file the \"embedded captions\"\n",
    "with open('captions_glove_embedded.pkl', 'wb') as file:\n",
    "    pkl.dump(vectors, file, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.2482962 -1.6422853 -0.3836474  0.2934045 -0.9514509 -0.6690476\n",
      " -2.812535 ]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(vectors):\n",
    "    if len(v) != 4:\n",
    "        print(i)\n",
    "        print(v)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40455,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \n\u001b[1;32m   1392\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1417\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1418\u001b[0m     X,\n\u001b[1;32m   1419\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1420\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1421\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1422\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1423\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1424\u001b[0m )\n\u001b[1;32m   1426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1428\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/sklearn/base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40455,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=30, random_state=0, n_init=\"auto\").fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.DataFrame(columns=[\"tags\"])\n",
    "tags[\"tags\"] = kmeans.labels_\n",
    "tags.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
