{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you runned wandb login?? OK. go aheadd...\n"
     ]
    }
   ],
   "source": [
    "from models_v4 import desperate_transformer\n",
    "from dataset import ImageCaptionDataset\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import yaml\n",
    "from typing import Dict\n",
    "import gensim.downloader as api\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "###SCRIPT CONFIG!####\n",
    "device = \"mps\" #r u running cuda my boy? or mps? :D\n",
    "num_epochs = 200\n",
    "batch_size = 50\n",
    "print(\"Have you runned wandb login?? OK. go aheadd...\")\n",
    "#####################\n",
    "\n",
    "#convert to dictionary a yaml \n",
    "def nested_dict(original_dict):\n",
    "    nested_dict = {}\n",
    "    for key, value in original_dict.items():\n",
    "        parts = key.split(\".\")\n",
    "        d = nested_dict\n",
    "        for part in parts[:-1]:\n",
    "            if part not in d:\n",
    "                d[part] = {}\n",
    "            d = d[part]\n",
    "        d[parts[-1]] = value\n",
    "    return nested_dict\n",
    "\n",
    "#load datasets\n",
    "with open('/Users/josepsmachine/Documents/UNI/DL/dlnn-project_ia-group_10/dataset/train_dataset.pkl', 'rb') as inp:\n",
    "    train_dataset = pickle.load(inp)\n",
    "with open('/Users/josepsmachine/Documents/UNI/DL/dlnn-project_ia-group_10/dataset/val_dataset.pkl', 'rb') as inp:\n",
    "    val_dataset = pickle.load(inp)\n",
    "#with open('/Users/josepsmachine/Documents/UNI/DL/dlnn-project_ia-group_10/dataset/debug_dataset.pkl', 'rb') as inp:\n",
    "#    debug_dataset = pickle.load(inp)\n",
    "\n",
    "#create dataloaders\n",
    "\n",
    "#debug_dataloader = DataLoader(debug_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#load word2vec pretrained embedding layer\n",
    "word2vec_emb = api.load('word2vec-google-news-300')\n",
    "word2vec_emb = torch.FloatTensor(word2vec_emb.vectors)\n",
    "\n",
    "#import vocabulary to word2vec indexes that we know\n",
    "with open('vocabidx2word2vecidx.pkl', 'rb') as inp:\n",
    "    vocabidx2word2vecidx = pickle.load(inp)\n",
    "\n",
    "with open('vocabulary.pkl', 'rb') as inp:\n",
    "    vocabulary = pickle.load(inp)\n",
    "\n",
    "word2vec_emb = word2vec_emb[vocabidx2word2vecidx]\n",
    "\n",
    "word2vec_emb = nn.Embedding.from_pretrained(word2vec_emb)\n",
    "word2vec_emb.requires_grad_ = False #freeze word2vec embeddding layer\n",
    "\n",
    "#load universal sentence encoder to define our own loss function\n",
    "sntc_enc = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#define cross entropy loss function\n",
    "cross_entrop = nn.CrossEntropyLoss(ignore_index=2074)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8090"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "desperate_transformer(\n",
       "  (embedding): Embedding(8426, 300)\n",
       "  (f_extr1): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (f_extr2): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (f_extr3): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (seq_mod1): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=335, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=335, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=335, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (seq_mod2): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (seq_mod3): AttentionBlock(\n",
       "    (key_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (val_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (query_gen): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (5): Softmax(dim=-1)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (LM_FC): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=8426, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer = word2vec_emb\n",
    "#else:\n",
    "    #the learnt embedding layer will have the same vocab size as word2vec for comparaison reasons.\n",
    "#emb_layer = nn.Embedding(num_embeddings=word2vec_emb.weight.shape[0], embedding_dim=300)\n",
    "\n",
    "model = desperate_transformer(2048, emb_layer, 300, 1)\n",
    "#model.init_weights()\n",
    "\n",
    "#print(config[\"see_once\"])\n",
    "#count_parameters(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "step_size = 1  # Number of epochs before adjusting the learning rate\n",
    "gamma =  0.0001\n",
    "def train(model):\n",
    "    with wandb.init(project=\"dl2023_imagecaptioning\", entity = \"dl2023team\") as run:\n",
    "        \n",
    "        run.name = f\"LSTM&miniTransformer_v4\"\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        ############\n",
    "        #Train loop:\n",
    "        ############\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            training_losses = [] # renamed from epoch_losses\n",
    "            progress_bar = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            for batch,(X1,X2,caption) in progress_bar:\n",
    "            #for batch,(X1,X2) in enumerate(train_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                X1 = X1.to(device) \n",
    "                X2 = X2.to(device)\n",
    "                out = model(X1,X2)\n",
    "                #ic(out.shape)\n",
    "                ref = X2\n",
    "                #change ref to join batch and sequence dim\n",
    "                ref = ref.reshape(ref.shape[0]*ref.shape[1])\n",
    "\n",
    "                #same for the out logits\n",
    "                #change ref to join batch and sequence dim\n",
    "                out = out.reshape(out.shape[0]*out.shape[1],out.shape[2])\n",
    "                \n",
    "                loss = cross_entrop(out,ref)\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "                optimizer.step()    \n",
    "                training_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({'Batch Loss': loss.item()})\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            average_training_loss = sum(training_losses) / len(training_losses) # renamed from avg_loss\n",
    "            wandb.log({'Train_Epoch_Loss': average_training_loss})\n",
    "\n",
    "            if average_training_loss < best_loss:\n",
    "                best_loss = average_training_loss\n",
    "                torch.save(model.state_dict(), \"LSTM&miniTransformer_v1.pt\")\n",
    "                wandb.save(f'LSTM&miniTransformer_v1.pt')\n",
    "                #print(f\"Model saved at {'{wandb.run.id}_LSTM&resnet18.pt'}\")\n",
    "        \n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharadai\u001b[0m (\u001b[33mdl2023team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/josepsmachine/Documents/UNI/DL/dlnn-project_ia-group_10/models/desperate_transformer/wandb/run-20230529_173356-a8ekicqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dl2023team/dl2023_imagecaptioning/runs/a8ekicqe' target=\"_blank\">fast-totem-112</a></strong> to <a href='https://wandb.ai/dl2023team/dl2023_imagecaptioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dl2023team/dl2023_imagecaptioning' target=\"_blank\">https://wandb.ai/dl2023team/dl2023_imagecaptioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dl2023team/dl2023_imagecaptioning/runs/a8ekicqe' target=\"_blank\">https://wandb.ai/dl2023team/dl2023_imagecaptioning/runs/a8ekicqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 397it [08:33,  1.29s/it, Batch Loss=8.97]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-totem-112</strong> at: <a href='https://wandb.ai/dl2023team/dl2023_imagecaptioning/runs/a8ekicqe' target=\"_blank\">https://wandb.ai/dl2023team/dl2023_imagecaptioning/runs/a8ekicqe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230529_173356-a8ekicqe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     27\u001b[0m X1 \u001b[38;5;241m=\u001b[39m X1\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     28\u001b[0m X2 \u001b[38;5;241m=\u001b[39m X2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#ic(out.shape)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m ref \u001b[38;5;241m=\u001b[39m X2\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UNI/DL/dlnn-project_ia-group_10/models/desperate_transformer/models_v4.py:115\u001b[0m, in \u001b[0;36mdesperate_transformer.forward\u001b[0;34m(self, x, x2)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_tok\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    114\u001b[0m     b_tok \u001b[39m=\u001b[39m n_tok[i]\n\u001b[0;32m--> 115\u001b[0m     b_tok \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(b_tok, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    116\u001b[0m     new_tokens\u001b[39m.\u001b[39mappend(b_tok[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    118\u001b[0m new_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(new_tokens)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m#create seq dim\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable =iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1,X2,caption = next(iterable)\n",
    "X1 = X1.to(device)\n",
    "X2 = X2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_tok(model,features,X2):\n",
    "    features = features.to(device)\n",
    "    tok = torch.tensor([[1]]).to(device)\n",
    "    caption = []\n",
    "    for i in range(X2.shape[1]):\n",
    "        out = model(X1,tok)\n",
    "        out = nn.functional.softmax(out)\n",
    "        gen_word = torch.multinomial(out[0,0], 1)\n",
    "        caption.append(gen_word[0])\n",
    "        tok = gen_word.unsqueeze(0)     \n",
    "    \n",
    "    return torch.tensor(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model,features):\n",
    "    features = features.to(device)\n",
    "    tok = torch.tensor([[1]]).to(device)\n",
    "    caption = \"bos\"\n",
    "    for i in range(40):\n",
    "        out = model(X1,tok)\n",
    "        out = nn.functional.softmax(out,dim=-1)\n",
    "        gen_word = torch.multinomial(out[0,0], 1)\n",
    "        caption += \" \"+vocabulary[gen_word]\n",
    "        tok = gen_word.unsqueeze(0)\n",
    "    \n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bos man and woman kissing eos',)\n",
      "bos sidecar prepairing swooping celebration brown mountaintops anything mastif tw boogie filled overcoat doorstep sushi encripted mercury nerf reclining physiques variety assisting scubba handkerchiefs cheer engraved old cylinder crooked threshold neckless flowered safe flag intently miasto sleeved windsurfing split tent headless\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(caption)\n",
    "print(generate_caption(model,X1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
