{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 324kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"<s> How are you?\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n> < do you feeling</'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = outputs[\"logits\"]\n",
    "generated_text = tokenizer.decode(torch.argmax(logit, dim=2).squeeze())\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello without trying to do so or by generation of information. 6.2. Requests for license adjustment There are mounting settings so that members of a licensee's \"Team Member Licensing Agreement Program\" (CDAP) are permitted contact information on the licensees' personal make contact devices for instance the EOFs. Applications to the CDAP also run over OpenVPN, which can also be further subject to the track record set by the firm using the vendeung server. The CDAP mandates appropriate\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the seed word\n",
    "seed_word = \"hello\"\n",
    "\n",
    "# Tokenize the seed word\n",
    "input_ids = tokenizer.encode(seed_word, return_tensors='pt')\n",
    "\n",
    "# Generate text iteratively\n",
    "generated_text = seed_word\n",
    "\n",
    "# Define the number of tokens to generate\n",
    "num_tokens = 100\n",
    "\n",
    "for _ in range(num_tokens):\n",
    "    # Generate the next token\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    next_token_id = torch.multinomial(next_token_probs, num_samples=1).item()\n",
    "    \n",
    "    # Append the generated token to the input\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=-1)\n",
    "    \n",
    "    # Decode the generated token and add it to the text\n",
    "    generated_token = tokenizer.decode(next_token_id)\n",
    "    generated_text += generated_token\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas on how to input image encoding to gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image features as initial tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image features -> (some block) -> word embedding dim \n",
    "\n",
    "We could have multiple embeddings as a sequence, each sequence could represent a feature extraction block in a grid along the image. Then keep the information of what grid it was in the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we bypass the embedding layer of gpt2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as child module 'wte' (torch.nn.Module or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m]])  \u001b[38;5;66;03m# Example: input tokens\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Replace the embedding layer output with the custom value\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m \u001b[38;5;241m=\u001b[39m custom_embedding_output\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform the forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_tokens)\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1653\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[39melif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1652\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot assign \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m as child module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1654\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39m(torch.nn.Module or None expected)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1655\u001b[0m                         \u001b[39m.\u001b[39mformat(torch\u001b[39m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1656\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m _global_module_registration_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m   1657\u001b[0m         output \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, name, value)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as child module 'wte' (torch.nn.Module or None expected)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "\n",
    "# Set the desired value to be fed as the output of the embedding layer\n",
    "custom_embedding_output = torch.randn(1, config.hidden_size)  # Example: random tensor\n",
    "\n",
    "# Set the input tokens for the forward pass\n",
    "input_tokens = torch.tensor([[5, 2, 7]])  # Example: input tokens\n",
    "\n",
    "# Replace the embedding layer output with the custom value\n",
    "model.transformer.wte = nn.Sequential(model.transformer.wte,) #\n",
    "\"\"\"\n",
    "    instead of sequential replace it with a model that takes the wte as an \n",
    "    init argument and then when an input is passed to it computes the image features \n",
    "    and adds it to the embedding output.\n",
    "\"\"\"\n",
    "# Perform the forward pass\n",
    "outputs = model(input_tokens)\n",
    "\n",
    "# Get the model output (logits, hidden states, etc.)\n",
    "logits = outputs.logits\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# Print the model output shapes\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Hidden states shape:\", hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proof that we are accessing the wte variable in the model class\n",
    "model.transformer.wte.forward(torch.tensor([0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
