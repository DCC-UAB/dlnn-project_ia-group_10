{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file = 'captions.txt'\n",
    "images_folder = 'Images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning project\n",
    "\n",
    "Objective - Generate natural language descriptions for images\n",
    "\n",
    "Type of models used:\n",
    "\n",
    "    - ResNet/Alexnet/VGG16 for extracting the image features\n",
    "\n",
    "    - ... for Text Encoding\n",
    "    \n",
    "    - LSTMs for text generation\n",
    "\n",
    "Data: Flickr 8k Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(image_name, captions_text):\n",
    "    captions = []\n",
    "    for line in captions_text.splitlines():\n",
    "        parts = line.split(',')\n",
    "        if len(parts) == 2 and parts[0] == image_name:\n",
    "            captions.append(parts[1])\n",
    "\n",
    "    return captions\n",
    "\n",
    "def build_captions_dict(captions_text):\n",
    "    captions_dict = {}\n",
    "    for line in captions_text.splitlines():\n",
    "        parts = line.split(',')\n",
    "        if len(parts) == 2:\n",
    "            if parts[0] not in captions_dict:\n",
    "                captions_dict[parts[0]] = []\n",
    "            captions_dict[parts[0]].append(parts[1])\n",
    "    return captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_images(folder_path, captions_dict, num_images_to_display=6, num_images_per_row=3, max_caption_length=40):\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    np.random.shuffle(file_names) \n",
    "    file_names = file_names[:num_images_to_display]  \n",
    "\n",
    "    num_images = len(file_names)\n",
    "    num_rows = (num_images + num_images_per_row - 1) // num_images_per_row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_images_per_row, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # If only one image is being displayed, axes won't be an array. We fix this here.\n",
    "    if num_images == 1:\n",
    "        axes = np.array([axes])\n",
    "        \n",
    "    axes = axes.flatten()  # Flatten the axes array for easier handling\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        img_path = os.path.join(folder_path, file_name)\n",
    "        img = plt.imread(img_path)\n",
    "        ax = axes[i]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        caption = captions_dict.get(file_name, [\"No caption\"])\n",
    "        if len(caption[0]) > max_caption_length:\n",
    "            caption_lines = [caption[0][0:max_caption_length], caption[0][max_caption_length:]]\n",
    "            ax.text(0.5, -0.1, caption_lines[0] + '\\n' + caption_lines[1], transform=ax.transAxes, fontsize=10, ha='center')\n",
    "        else:\n",
    "            ax.text(0.5, -0.1, caption[0], transform=ax.transAxes, fontsize=10, ha='center')\n",
    "\n",
    "    for j in range(num_images, num_rows * num_images_per_row):\n",
    "        fig.delaxes(axes[j])  # Remove the extra sub-plots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(captions_file, \"r\") as file:\n",
    "    captions_text = file.read()\n",
    "captions_dict = build_captions_dict(captions_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(folder_path=images_folder, captions_dict=captions_dict, num_images_to_display=9, num_images_per_row=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook will consist of 3 main sections, similarly to the functionality of the code.\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Feature extraction from images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with applying some transformations to the input images.\n",
    "\n",
    "Precomputed the Mean and Standard Deviation:\n",
    "\n",
    "Mean: [0.45802852 0.4460975  0.40391668]\n",
    "\n",
    "Standard Deviation: [0.24219123 0.2332004  0.23719894]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.45802852, 0.4460975, 0.40391668]\n",
    "std = [0.24219123, 0.2332004, 0.23719894]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # Resize the image: ResNet model  - > (224,224,3)\n",
    "    transforms.ToTensor(), # Img to Python Tensor\n",
    "    transforms.Normalize(mean=mean, std=std), # image = (image - mean) / std\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the applied transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "random_file = random.choice(file_names) # Pick one image at random\n",
    "\n",
    "image_path = os.path.join(images_folder, random_file)\n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "normalized_image = preprocess(original_image)\n",
    "normalized_image = torch.clamp(normalized_image, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# Original Image\n",
    "axs[0].imshow(original_image)\n",
    "axs[0].set_title(\"Original Image\")\n",
    "\n",
    "# Preprocessed Image\n",
    "axs[1].imshow(normalized_image.permute(1, 2, 0))\n",
    "axs[1].set_title(\"Processed Image\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the features from the augmented images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pretrained weights for now seems like a good idea. Especially considering the fact that the weights are from ImageNet which is a large and diverse dataset that contains a wide variety of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature extraction model\n",
    "\n",
    "\"\"\"\n",
    "\"pretrained=True\" argument - > Pre-trained weights for ResNet-18. \n",
    " By default - > Weights trained on the ImageNet dataset.\n",
    "\n",
    "\"\"\"\n",
    "def load_model(model_name='resnet18'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        model_func = getattr(models, model_name)\n",
    "        model = model_func(pretrained=True)\n",
    "    except KeyError:\n",
    "        raise ValueError(f'{model_name} is not a valid model name.')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model('resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(images_path, model_name, batch_size=64):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "    image_files = os.listdir(images_path)\n",
    "    dataloader = DataLoader(image_files, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    features = {}\n",
    "    for batch in tqdm(dataloader, total=len(image_files)//batch_size):\n",
    "        batch_images = []\n",
    "        for img_name in batch:\n",
    "            if img_name.endswith('.jpg'):\n",
    "                try:\n",
    "                    img = Image.open(os.path.join(images_path, img_name)).convert('RGB')\n",
    "                    img = preprocess(img).unsqueeze(0).to(device)\n",
    "                    batch_images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping image {img_name} due to error: {e}\")\n",
    "                    continue\n",
    "        batch_images = torch.cat(batch_images, 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model = load_model(model_name)\n",
    "            feature = model(batch_images)\n",
    "        \n",
    "        for img_name, feature in zip(batch, feature.cpu()):\n",
    "            features[img_name] = torch.flatten(feature).numpy()\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_image_features(images_folder, model_name = 'resnet18')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_uab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
